---
title: "MSDS6372 Project 2 - Osteoporosis in Women"
author: "Caroll Rodriguez, Rajat Chandna, Randall Hendrickson, Lokesh Maganti"
date: "August 18, 2018"
output: 
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    keep_md: yes
---
<style type="text/css">

body, td {
   font-size: 16px;
}
code.r{
  font-size: 8px;
}
pre {
  font-size: 10px
}
/* code.r will control the font size for R code echoed from the code chunk, while pre will apply to any R results output from the code. */
</style>

```{r setup00, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(knitr)
library(kableExtra)

```
\newpage

## **Objective 1** - EDA and logistic regression model


## Introduction

The Global Longitudinal Study of Osteoporosis in Women (GLOW) (2005-2014) was a prospective cohort study of physician practices in the provision of prophylaxis and treatment against osteoporotic fractures. The goal of this research was to improve understanding of the risk and prevention of osteoporosis-related fractures among female residents of 10 countries who were 55 years of age and older. GLOW enrolled over 60,000 women through over 700 physicians in 10 countries, and conducted annual follow-up for up to 5 years through annual patient questionnaires.

The aim of the GLOW study was to collect uniform data to help describe the distribution of risk factors for osteoporosis-related fracture. This analysis uses one dataset from this study to try to predict a fracture using these risk factors.


## Data Description
<p>The data set provided is about predicting whether a woman with osteoporosis will have another bone fracture.  Of course getting a bone fracture is somewhat circumstantial, but with this disease every day life could trigger a break if the progression of the disease is strong.  

The dataset included a total of 14 variables: 3 ID variables which tell us the subject, doctor and physical location of each record, 4 continuous variables (BMI, Weight, Height, and Age), 6 categorical variables (PRIORFRAC, PREMENO, MOMFRAC, PREMEO, MOMFRAC, ARMASSIST, SMOKE, RATERISK), and the response (FRACTURE). We were unable to find a mapping the subjects with their location to understand the mix of countries represented.

We have 500 subjects in the dataset of which 33% of the subjects have/had fractures. 

Missing values were not detected in dataset. Special characters were removed from column headings.
What we know/don't know about the sample (500)


## Exploratory Analysis  

### Assumptions  
This is a prospective study which means it's a study over time of a group of similar individuals
who differ with respect to certain factors under a study and how these factors affect rates of a certain outcome
(Fracture vs No-Fracture) Linearity - Independence of errors - Based on SUB_ID(Subject ID) we confirm
each record is an independent sample. Multicollinearity - Weight and BMI are highly correlated but we will
remove one from the analysis.


```
GLOW dataset:

Variable Name Type   #Unique  

     SUB_ID  integer  500 - Identification Code (1 - n)  
    SITE_ID  integer    6 - Study Site (1 - 6)  
     PHY_ID  integer  127 - Physician ID code (128 unique codes)  
 PRIORFRAC*   factor    2 - History of Prior Fracture (1: No, 2: Yes)   
        AGE  integer   36 - Age at Enrollment (Years)  
     WEIGHT  numeric  128 - Weight at enrollment (Kilograms)  
     HEIGHT  integer   34 - Height at enrollment (Centimeters)  
        BMI  numeric  409 - Body Mass Index (Kg/m^2)  
   PREMENO*   factor    2 - Menopause before age 45 (1: No, 2: Yes)  
   MOMFRAC*   factor    2 - Mother had hip fracture (1: No, 2: Yes)  
 ARMASSIST*   factor    2 - Arms are needed to stand from a chair (1: No, 2: Yes)  
     SMOKE*   factor    2 - Former or current smoker (1: No, 2: Yes)  
  RATERISK*   factor    3 - Self-reported risk of fracture (1: Less, 2: Same, 3: Greater)    
  FRACSCORE  integer   12 - Fracture Risk Score (Composite Risk Score)  
  FRACTURE*   factor    2 - Any fracture in first year (1: No, 2: Yes)  
  
```
```{r, echo = FALSE, out.width="25%"}
myimages<-list.files("./eda_images/continuous", pattern = ".png", full.names = TRUE)
include_graphics(myimages)
```

#### Figure 1 - Boxplots for Continuous Variables AGE, BMI, HEIGHT, WEIGHT

In Figure 1, we see the boxplots for the continous variables AGE, WEIGHT, HEIGHT, BMI.  

```{r, echo = FALSE, out.width="25%"}
myimages<-list.files("./eda_images/continuous_visual", pattern = ".png", full.names = TRUE)
include_graphics(myimages)
```

#### Figure 2 - Correlation, and Density plots for Continuous Variables AGE, BMI, HEIGHT, WEIGHT

```{r, echo = FALSE, out.width="50%"}
myimages<-list.files("./eda_images/multivariate", pattern = ".png", full.names = TRUE)
include_graphics(myimages)
```

#### Figure 3 - Barplot (occurrances) and Multivariate Plots for Categorical and Continuous Variables 


```{r, echo = FALSE, out.width="50%"}
myimages<-list.files("./eda_images/scatterplots", pattern = ".png", full.names = TRUE)
include_graphics(myimages)
```

#### Figure 4 - Scatterplots 

```{r, echo = FALSE, out.width="25%"}
myimages<-list.files("./eda_images/roc_and_2way", pattern = ".png", full.names = TRUE)
include_graphics(myimages)
```

#### Figure 5 - ROC and 2-way Tables 

```{r, echo = FALSE, out.width="50%"}
myimages<-list.files("./eda_images/clustering", pattern = ".png", full.names = TRUE)
include_graphics(myimages)
```

#### Figure 6 - Clustering  



## Restatement of Problem and the overall approach to solve it.

Logistic regression is used to describe data and to explain the relationship between one dependent binary variable, in this case whether a woman will have a fracture related to osteoporosis, with one or more continuous or categorical variables. Using different modeling techniques, we will try to predict whether a sample will have a fracture related event.

## **Simple Logistic Model Selection**


**Model Considerations**

For the purpose of feature selection for the simple logistic model, a lasso+logistic regression with cross validation (for 1000 lambda values) was performed on training data set in order to obtain the penalty value(lambda) that results in minimum misclassification rate. The result of this procedure is present in figure 7 below:

```{r, echo = FALSE, out.width="33%"}
myimages<-list.files("./model_comparison_images/lr_assumptionscheck", pattern = ".png", full.names = TRUE)
include_graphics(myimages)
```

#### Figure 7 - Simple Logistic Regression lasso+logistic regression  

```
Then the lasso+logistic model was rerun with best value of lambda obtained(lambda.min) from previous procedure and was used to uncover feature set that was later used as initial starting point in our simple logistic model. The features selected after LASSO shrinkage procedure are as:
12 x 1 sparse Matrix of class "dgCMatrix"
                                  1
(Intercept)              1.52902669
trainingData.AGE         0.03598544
trainingData.WEIGHT      .         
trainingData.HEIGHT     -0.03340871
trainingData.BMI         .         
trainingData.PRIORFRAC1  0.15180349
trainingData.PREMENO1    .         
trainingData.MOMFRAC1    0.04035537
trainingData.ARMASSIST1  0.52512963
trainingData.SMOKE1      .         
trainingData.RATERISK.L  0.33991586
trainingData.RATERISK.Q  . 
```

Next, we went ahead and run simple logistic regression using this feature set of AGE, HEIGHT, PRIORFRAC, MOMFRAC, ARMASSIST and RATERISK.


**Model Assumptions:**  

**Assumption of binary response while running binary logistic regression.**  

The dependent variable is a factor with two defined levels (0 = No, 1 = Yes).  

**Assumption of independence among observations.**  

Since the method for selecting the subjects for this study and then formulation of given dataset from all of such population is not fully known, caution must be taken while generalizing the results from this analysis. Potential biases could be present among observations as selection bias, recall bias, serial and spatial correlation etc could be present. Generalizing the results from this analysis to whole population of such subjects is to be based upon assumption that subjects in given dataset are as representative of the underlying population as a random samples from such 
population are. For the purpose of our analysis, we assume observations in our dataset are independent of one another and proceed with the analysis.

**Assumption of linearity of independent continuous predictors and their respective log odds**  

A scatter plot between two continuous predictors: AGE and HEIGHT as identified to be used in simple logistic model and their respective log odds is plotted and present as below.

```{r, echo = FALSE, out.width="75%"}
include_graphics("./model_comparison_images/lr_normal/logisticRegressionNormal1-3.png")
```

#### Figure X - Scatter Plot between AGE and HEIGHT and their log odds    

**Simple Logistic Model Fit:**  

The overall logistic regression model using selected variables came out significant and found AGE, HEIGHT, ARMASSIST and RATERISK as significant predictors at alpha=0.05 level for determining probability of getting a fracture in first year(response variable). The model output is as below:  

```
Call:
glm(formula = FRACTURE ~ AGE + HEIGHT + PRIORFRAC + MOMFRAC + 
    ARMASSIST + RATERISK, family = binomial(link = "logit"), 
    data = trainingData)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5491  -0.7377  -0.5763   0.2298   2.2214  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)   
(Intercept)  3.33365    3.80104   0.877  0.38047   
AGE          0.04347    0.01578   2.755  0.00587 **
HEIGHT      -0.04881    0.02165  -2.254  0.02418 * 
PRIORFRAC1   0.22281    0.30097   0.740  0.45912   
MOMFRAC1     0.33522    0.38263   0.876  0.38097   
ARMASSIST1   0.68418    0.27861   2.456  0.01406 * 
RATERISK.L   0.50762    0.24656   2.059  0.03951 * 
RATERISK.Q  -0.06727    0.22219  -0.303  0.76209   
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 395.31  on 350  degrees of freedom
Residual deviance: 355.55  on 343  degrees of freedom
AIC: 371.55
```
A recursive model refinement process was performed by trying to add more variables that were not present in initial fit; and model performance in terms of AUC under ROC curve and accuracy via confusion matrix for validation dataset was assessed for each subsequent model. No subsequent model fit resulted in significant gains in terms of improved performance parameters. In fact, some resulted in decrease in accuracy on validation set as they were just acting as noise and didn’t provide any valuable information about the response variable. Hence, initial fit was chosen as best fit and assessed further.   

On the initial model fit, VIFs and influential observation analysis was then performed and results can be seen below as:

```{r, echo = FALSE, out.width="75%"}
include_graphics("./model_comparison_images/lr_normal/logisticRegressionNormal-cooksD.png")
```

```
              GVIF Df GVIF^(1/(2*Df))
AGE       1.209972  1        1.099987
HEIGHT    1.084342  1        1.041317
PRIORFRAC 1.172110  1        1.082640
MOMFRAC   1.010590  1        1.005281
ARMASSIST 1.124261  1        1.060312
RATERISK  1.096380  2        1.023270

```

As seen from above results that multiclonality is not seen and no substantially influential observation is found as seen from Cook’s d plot. Hence, we proceeded with current model for model equation formulation and coefficients interpretations.


**Model Equation**

$log(\pi)$ = 3.33365 +  0.04347 * AGE - 0.04881 * HEIGHT + 0.22281 * PRIORFRAC + 0.33522 * MOMFRAC    
                + 0.68418 * ARMASSIST + 0.50762 * RATERISK_2 - 0.06727 * RATERISK_3    

```
Next, we converted model coefficients and their respective 95% Confidence Intervals to Normal Scale and proceeded with coefficient interpretations as below:  

            ODDs_Ratio     2.5 %       97.5 %
(Intercept) 28.0403767 0.0172620 5.378401e+04
AGE          1.0444253 1.0128322 1.077662e+00
HEIGHT       0.9523628 0.9118527 9.929059e-01
PRIORFRAC1   1.2495774 0.6857875 2.237992e+00
MOMFRAC1     1.3982464 0.6449924 2.917750e+00
ARMASSIST1   1.9821443 1.1469582 3.428352e+00
RATERISK.L   1.6613370 1.0283916 2.713240e+00
RATERISK.Q   0.9349462 0.6060378 1.451499e+00

```


## *Interpretation of the Coefficients:*  


For AGE: All the other variables being constant, for every one-year increase in age of women, the odds of being getting a fracture in first year (versus not getting a fracture in first year) increases by a factor of 1.04(4% increase). The 95% Confidence Interval for this multiplicative factor is from 1.01(1% increase) to 1.08(8% increase).  

For HEIGHT: All the other variables being constant, for every one unit increase in height of women, the odds of being getting a fracture in first year (versus not getting a fracture in first year) decreases by a factor of 0.95(5% decrease). The 95% Confidence Interval for this multiplicative factor is from 0.91(9% decrease) to 0.99(1% decrease).  

** All the below coefficients are of format:
The estimated odds for Person X with/without characteristic are M times the odds (of developing fracture in first year), for another Person Y without/with that characteristic. **  

For PRIORFRAC: All the other variables being constant, the estimated odds for a woman, who has history of prior fracture, are 1.25 (25% more) times the odds of having fracture again in first year, for a woman who didn’t have prior history of fracture. The 95% Confidence Interval for this estimated odds ratio is from 0.69 times to 2.23 times.

For MOMFRAC: All the other variables being constant, the estimated odds for a woman, whose mother had hip fracture, are 1.40 (40% more) times the odds of having fracture in first year, for a woman whose mother didn’t have hip fracture. The 95% Confidence Interval for this estimated odds ratio is from 0.64 times to 2.91 times.

For ARMASSIST: All the other variables being constant, the estimated odds for a woman, who needed arms to stand from a chair, are 1.98 (98% more) times the odds of having fracture in first year, for a woman who didn’t need arms to stand from a chair. The 95% Confidence Interval for this estimated odds ratio is from 1.15 times to 3.43 times.

For RATERISK.2: All the other variables being constant, the estimated odds for a woman, who self-reported that her risk for developing fracture is same as others of the same age, are 1.66 (66% more) times the odds of having fracture in first year, for a woman who self-reported that her risk for developing fracture is less than others of the same age. The 95% Confidence Interval for this estimated odds ratio is from 1.03 times to 2.71 times.

For RATERISK.3: All the other variables being constant, the estimated odds for a woman, who self-reported that her risk for developing fracture is greater than others of the same age, are 0.93 (7% less) times the odds of having fracture in first year, for a woman who self-reported that her risk for developing fracture is less than others of the same age. The 95% Confidence Interval for this estimated odds ratio is from 0.61 times to 1.45 times.



**Model Assessment:**  

Model performance was assessed on validation data using following parameters:
1.	AUC under ROC curve
2.	Overall Accuracy, Sensitivity and Specificity values obtained from confusion matrix.


```{r, echo = FALSE, out.width="50%"}
pics<-c("./model_comparison_images/lr_normal/logisticRegressionNormal-1.png", "./model_comparison_images/lr_normal/logisticRegressionNormal-2.png")

include_graphics(pics)
```

```
## Confusion Matrix and Statistics
##
## Reference
## Prediction 0 1
## 0 108 35
## 1 4 2
##
## Accuracy : 0.7383
## 95% CI : (0.66, 0.8068)
## No Information Rate : 0.7517
## P-Value [Acc > NIR] : 0.6866
##
## Kappa : 0.0255
## Mcnemar's Test P-Value : 1.556e-06
##
## Sensitivity : 0.96429
## Specificity : 0.05405
## Pos Pred Value : 0.75524
## Neg Pred Value : 0.33333
## Prevalence : 0.75168
## Detection Rate : 0.72483
## Detection Prevalence : 0.95973
## Balanced Accuracy : 0.50917
##
## 'Positive' Class : 0
##
```

As seen from above ROC curves and confusion matrix:

1.	Training set AUC is 71.6% and that for validation data set is about 67.7%. This is understandable since model was created using data training data set.  

2.	Overall Accuracy of the model is at 74.5% with high sensitivity 94.64% but very low specificity 13.51 %.   

3.	There relative low values of overall AUC and accuracy could be due to:  
    a.	Lack of complexity in the current model (in terms of higher order terms, transformations, interactions) etc.  
    b.	The available feature set and number of samples available are not sufficient enough to accurate model most of the trends in actual population data.  
    c.	Class imbalance that is present in original, training and validation data set. Especially, the number of true positive cases are very much under represented in both the training and test datasets and this could be the cause of low specificity values obtained from this model.  
    
4.	 Specificity for the model could be improved by lower the cutoff for classification from its initial value of 0.5. This should be warranted since cost of not predicting true positive outweighs cost of predicting false positive in the current situation.  



## Final conclusions from the analysis of Objective 1

To improve the accuracy and AUC for the model, we would next increase complexity in current model by adding interactions to it.






\newpage

## **Objective 2** - additional competing models

•	Record the predictive performance metrics from your simple, highly interpretable model from Objective 1.  


•	You must include one additional model which is also a more complicated logistic regression model than in Objective 1.  By complicated, I do not mean that you include more predictors (that will be somewhat sorted out in Objective 1), but rather model complexity through interaction terms, new variables created by the group, or transformations.

•	Create another competing model using just the continuous predictors and use LDA or QDA.

•	Use a nonparameteric model approach as a competing model.  Random forest for predictors that are both categorical and continuous or a k-nearest neighbors approach if just working with continuous predictors. 

•	Provide a summary table of the performance across the competing methods. Summarize the overall findings.  A really great report will also give insight as to why the “best” model won out.  This is where a thorough EDA will always help.

## Visualizations from Model Comparisions

```{r, echo = FALSE, out.width="50%"}
myimages<-list.files("./model_comparison_images/fracture_counts_cv", pattern = ".png", full.names = TRUE)
include_graphics(myimages)
```

#### Figure 7 - Fracture Counts  


```{r, echo = FALSE, out.width="25%"}
myimages<-list.files("./model_comparison_images/lr_normal", pattern = ".png", full.names = TRUE)
include_graphics(myimages)
```

#### Figure 9 - Logistic Regression Normal - Train, Test, Linear, Cooks  



```{r, echo = FALSE, out.width="25%"}
myimages<-list.files("./model_comparison_images/plotting_interactions", pattern = ".png", full.names = TRUE)
include_graphics(myimages)
```

#### Figure 10 - Logistic Regression - Interactions  


```{r, echo = FALSE, out.width="25%"}
myimages<-list.files("./model_comparison_images/random_forests", pattern = ".png", full.names = TRUE)
include_graphics(myimages)
```

#### Figure 11 - Logistic Regression - Random Forests  


```{r, echo = FALSE, out.width="25%"}
myimages<-list.files("./model_comparison_images/conditional_random_forests", pattern = ".png", full.names = TRUE)
include_graphics(myimages)
```

#### Figure 12 - Logistic Regression - Conditional Random Forests  

```{r, echo = FALSE, out.width="25%"}
myimages<-list.files("./model_comparison_images/LDA", pattern = ".png", full.names = TRUE)
include_graphics(myimages)
```

#### Figure 13 - LDA  


## End Visualizations from Model Comparison

## Record predictive performance from objective 1

## Another competing model (interaction terms)

## Another competing model (just continuous) LDA

## nonparametric model (random forest)

## another nonparametric model (conditional random forest)

## Summary table of performance

| Model                                                         | Predictors         | Accuracy | 95% CI         | Sensitivity | Specificity | AUC   |
|---------------------------------------------------------------|--------------------|----------|----------------|-------------|-------------|-------|
| Logistic Regression (logit)                                   | 7                  | 74.5%    | (66.7%, 81.3%) | 94.6%       | 13.5%       | 67.8% |
| Logistic Regression w/Interactions (logit)                    | 7 + 3 interactions | 75.2%    | (67.4%, 81.9%) | 92.9%       | 21.6%       | 72.2% |
| Random Forest*                                                | 10                 | 74.5%    | (66.7%, 81.3%) | 99.1%       | 0.0%        | 74.2% |
| *RF Lower Cutoff (decreasing the probability from 50% to 30%) | 10                 | 73.8%    | (66%, 80.7%)   | 91.1%       | 21.6%       |       |
| Conditional Random Forest                                     | 10                 | 75.2%    | (67.4%, 81.9%) | 94.6%       | 16.2%       | 69.3% |
| LDA                                                           | 4                  | 73.2%    | (65.3%, 80.1%) | 94.6%       | 8.1%        | 60.2% |


## Conclusion/Discussion

TODO:


## ***Appendix*** ==========================================  


## *** ***Appendix A:*** EDA - Analysis =======================  


```{r child = 'EDA.Rmd'}
```


## *** ***Appendix B:*** Model Comparison - Analysis ==========  


```{r child = 'ModelComparison.Rmd'}
```


## *** ***Appendix C:*** Test interaction - LDA ===============  


```{r child = 'test_interaction.Rmd'}
```

